{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Persistent Homology\n",
    "\n",
    "## &copy;  [Omkar Mehta](omehta2@illinois.edu) ##\n",
    "### Industrial and Enterprise Systems Engineering, The Grainger College of Engineering,  UIUC ###\n",
    "\n",
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# [Reference](https://towardsdatascience.com/how-to-pull-data-from-an-api-using-python-requests-edcc8d6441b1)\n",
    "\n",
    "# Part 1: Download Data from INaturalist website using API\n",
    "From API documentation,we get the following information:\n",
    "\n",
    "`Please note that we throttle API usage to a max of 100 requests per minute, though we ask that you try to keep it to 60 requests per minute or lower, and to keep under 10,000 requests per day. If we notice usage that has serious impact on our performance we may institute blocks without notification.`\n",
    "\n",
    "`per_page\n",
    "Allowed values: 1 to 200`\n",
    "\n",
    "Locations data collected from:\n",
    "* place_id: 1563\n",
    "* place_id: 49906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for storing all the pulled data\n",
    "data = {\n",
    "    'time_observed_at': list(),\n",
    "    'species_guess': list(),\n",
    "    'genus_name': list(),\n",
    "    'rank': list(),\n",
    "    'wikipedia_url': list(),\n",
    "    'iconic_taxon_name': list(),\n",
    "    'preferred_common_name': list(),\n",
    "    'uri': list(),\n",
    "    'longitude': list(),\n",
    "    'latitude': list(),\n",
    "    'place_guess': list()\n",
    "}"
   ]
  },
  {
   "source": [
    "## Part 1.1. Function for getting the dictionary of pages and page_numbers\n",
    "\n",
    "For each page, there is a different per_page limit. This was not mentioned in the api documentation of the inaturalist website."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = {}\n",
    "# place_id = 1563\n",
    "# page_number = 1\n",
    "# per_page = 200 #max_limit\n",
    "# i = 1\n",
    "# sleep_time = 60\n",
    "# while True:\n",
    "#   while per_page != 0:\n",
    "#     #go through each page and per_page, get the request's response, if it is 200, append page_number:per_page to the dictionary.\n",
    "#     #if for any per_page, we get status_code != 200, we decrement the per_page by 1, and check response. \n",
    "    \n",
    "#     response = requests.get(\"https://api.inaturalist.org/v1/observations?place_id={}&page={}&per_page={}\".format(place_id, page_number, per_page))\n",
    "#     if i%50 == 0: #We have a limit of 60 pages per minute, for pulling data from API\n",
    "#         time.sleep(sleep_time)\n",
    "#     if response.status_code == 200:\n",
    "#       pages[page_number] = per_page\n",
    "#       page_number +=1\n",
    "#     elif response.status_code != 200:\n",
    "#       per_page -= 1\n",
    "#     i += 1\n",
    "#   break \n",
    "\n",
    "def findPerPage(place_id, page_number, per_page, how_many_pages):\n",
    "    pages = {}\n",
    "    i = 1\n",
    "    sleep_time = 60\n",
    "    while True:\n",
    "        while per_page != 0:\n",
    "            #go through each page and per_page, get the request's response, \n",
    "            # if it is 200, append page_number:per_page to the dictionary.\n",
    "            # if for any per_page, we get status_code != 200, we decrement the per_page by 1, and check response. \n",
    "            response = requests.get(\"https://api.inaturalist.org/v1/observations?place_id={}&page={}&per_page={}\".format(place_id, page_number, per_page))\n",
    "            if i%50 == 0: #We have a limit of 60 pages per minute, for pulling data from API\n",
    "                time.sleep(sleep_time)\n",
    "            if response.status_code == 200:\n",
    "                pages[page_number] = per_page\n",
    "                page_number +=1\n",
    "            elif response.status_code != 200:\n",
    "                per_page -= 1\n",
    "            i += 1\n",
    "\n",
    "            # if how_many_pages >= 200:\n",
    "            #     return pages\n",
    "        break\n",
    "    return pages\n",
    "\n",
    "def download_csv_pages(pages):\n",
    "    pages_Series = pd.Series(pages)\n",
    "    #pages_Series.to_csv('pages.csv')\n",
    "    pages_df = pd.DataFrame(pages_Series, columns=['page_number', 'per_page'])\n",
    "    pages_df.to_csv('pages.csv')\n",
    "\n",
    "def download_pickle_pages(pages):\n",
    "    pages_Series = pd.Series(pages)\n",
    "    #pages_Series.to_csv('pages.csv')\n",
    "    pages_df = pd.DataFrame(pages_Series, columns=['page_number', 'per_page'])\n",
    "    pages_df.to_pickle('pages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages_Series = pd.Series(pages)\n",
    "# pages_Series.to_csv('pages.csv')\n",
    "# pages_df = pd.read_csv('pages.csv', names=['page_number', 'per_page'])\n",
    "# pages_df.info()\n",
    "# pages_df.to_csv('pages.csv')\n",
    "# pages_df.to_pickle('pages.pkl')"
   ]
  },
  {
   "source": [
    "## Part 1.2. Pull data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pulling the data\n",
    "def pull_data(data, place_id, sleep_time, page_number, per_page):\n",
    "    '''\n",
    "    place_id: each location has a place_id, which we can get from the website\n",
    "    sleep_time: time in seconds we want to sleep\n",
    "    page_number: depends on the number of observations. total_observations/per_page\n",
    "    per_page: #observations per page\n",
    "    '''\n",
    "    if page_number%40 == 0:\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        response = requests.get(\"https://api.inaturalist.org/v1/observations?place_id={}&page={}&per_page={}\".format(place_id, page_number, per_page))\n",
    "        file_dict = response.json()\n",
    "        if ('results' in file_dict):\n",
    "            for j in range(per_page):\n",
    "            \n",
    "                if 'time_observed_at' in file_dict['results'][j]:\n",
    "                    data['time_observed_at'].append(file_dict['results'][j]['time_observed_at'])\n",
    "                else:\n",
    "                    data['time_observed_at'].append(None)\n",
    "                if 'species_guess' in file_dict['results'][j]:\n",
    "                    data['species_guess'].append(file_dict['results'][j]['species_guess'])\n",
    "                else:\n",
    "                    data['species_guess'].append(None)\n",
    "\n",
    "                if ('taxon' in file_dict['results'][j]) & (file_dict['results'][j]['taxon'] is not None) :\n",
    "                    #print(j)\n",
    "                    if 'name' in file_dict['results'][j]['taxon']:\n",
    "                        data['genus_name'].append(file_dict['results'][j]['taxon']['name'])\n",
    "                    else:\n",
    "                        data['genus_name'].append(None)\n",
    "                    if 'rank' in file_dict['results'][j]['taxon']:\n",
    "                        data['rank'].append(file_dict['results'][j]['taxon']['rank'])\n",
    "                    else:\n",
    "                        data['rank'].append(None)\n",
    "                    if 'wikipedia_url' in file_dict['results'][j]['taxon']:\n",
    "                        data['wikipedia_url'].append(file_dict['results'][j]['taxon']['wikipedia_url'])\n",
    "                    else:\n",
    "                        data['wikipedia_url'].append(None)\n",
    "                    if 'iconic_taxon_name' in file_dict['results'][j]['taxon']:\n",
    "                        data['iconic_taxon_name'].append(file_dict['results'][j]['taxon']['iconic_taxon_name'])\n",
    "                    else:\n",
    "                        data['iconic_taxon_name'].append(None)\n",
    "                    #print(j)\n",
    "                    if 'preferred_common_name' in file_dict['results'][j]['taxon']:\n",
    "                        data['preferred_common_name'].append(file_dict['results'][j]['taxon']['preferred_common_name'])\n",
    "                    else:\n",
    "                        data['preferred_common_name'].append(None)\n",
    "                else:\n",
    "                    data['genus_name'].append(None)\n",
    "                    data['rank'].append(None)\n",
    "                    data['wikipedia_url'].append(None)\n",
    "                    data['iconic_taxon_name'].append(None)\n",
    "                    data['preferred_common_name'].append(None)\n",
    "                if 'uri' in file_dict['results'][j]:\n",
    "                    data['uri'].append(file_dict['results'][j]['uri'])\n",
    "                else:\n",
    "                    data['uri'].append(None)\n",
    "                if 'geojson' in file_dict['results'][j]:\n",
    "                    data['longitude'].append(file_dict['results'][j]['geojson']['coordinates'][0])\n",
    "                    data['latitude'].append(file_dict['results'][j]['geojson']['coordinates'][1])\n",
    "                else:\n",
    "                    data['longitude'].append(None)\n",
    "                    data['latitude'].append(None)\n",
    "                if 'place_guess' in file_dict['results'][j]:\n",
    "                    data['place_guess'].append(file_dict['results'][j]['place_guess'])\n",
    "                else:\n",
    "                    data['place_guess'].append(None)\n",
    "        else:\n",
    "            print(page_number)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "`pages.csv' contains the \n",
    "```python\n",
    "{'page_number': 'per_page'}\n",
    "```\n",
    "dictionary. For each page, we have a maximum limit on the number of observations that one can pull."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for pulling data. Change place_id\n",
    "# for i in range(1, len(pages)):\n",
    "\n",
    "#     pull_data(data, 1563, 60, int(pages['page_number'][i]), int(pages['per_page'][i]))\n",
    "# data_df = pd.DataFrame(data)\n",
    "# data_df.info()\n",
    "\n",
    "# place_id = 1563\n",
    "# csv_file = 'data_' + str(place_id) + '_' + str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S')) + '.csv'\n",
    "# pickle_file = 'data_' + str(place_id) + '_' + str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S')) + '.pkl'\n",
    "# data_df.to_csv(csv_file)\n",
    "# data_df.to_pickle(pickle_file)\n",
    "# data_df.info()\n",
    "\n",
    "def getData(pages_df, data, place_id):\n",
    "\n",
    "    for i in range(1, len(pages)):\n",
    "        pull_data(data, place_id, 60, int(pages_df['page_number'][i]), int(pages_df['per_page'][i]))\n",
    "    data_df = pd.DataFrame(data)\n",
    "    return data_df\n",
    "def download_csv_data(data, place_id, ):\n",
    "\n",
    "    csv_file = 'data_' + str(place_id) + '_' + str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S')) + '.csv'\n",
    "    pickle_file = 'data_' + str(place_id) + '_' + str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S')) + '.pkl'\n",
    "    data_df.to_csv(csv_file)\n",
    "    data_df.to_pickle(pickle_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Use pickle instead of csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 45576 entries, 0 to 45575\nData columns (total 12 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Unnamed: 0             45576 non-null  int64  \n 1   time_observed_at       45240 non-null  object \n 2   species_guess          31406 non-null  object \n 3   genus_name             44627 non-null  object \n 4   rank                   44627 non-null  object \n 5   wikipedia_url          42470 non-null  object \n 6   iconic_taxon_name      44611 non-null  object \n 7   preferred_common_name  42654 non-null  object \n 8   uri                    45576 non-null  object \n 9   longitude              45576 non-null  float64\n 10  latitude               45576 non-null  float64\n 11  place_guess            45576 non-null  object \ndtypes: float64(2), int64(1), object(9)\nmemory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data_1563_2021_06_24_12_57_28.csv')\n",
    "# data_df.to_pickle('data_1563_2021_06_06_20_47_20.pickle')\n",
    "# data_df = pd.read_pickle('data_1563_2021_06_06_20_47_20.pickle')\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0           time_observed_at       species_guess  \\\n",
       "0           0  2021-06-23T16:22:12-05:00                 NaN   \n",
       "1           1  2021-06-24T10:29:25-05:00                 NaN   \n",
       "2           2  2021-06-23T14:19:35-05:00                 NaN   \n",
       "3           3  2021-06-24T09:45:31+00:00                 NaN   \n",
       "4           4  2021-06-24T06:03:52-05:00  Eastern Box Turtle   \n",
       "\n",
       "                    genus_name        rank  \\\n",
       "0              Storeria dekayi     species   \n",
       "1         Potentilla norvegica     species   \n",
       "2                Tettigoniinae   subfamily   \n",
       "3             Marpissa lineata     species   \n",
       "4  Terrapene carolina carolina  subspecies   \n",
       "\n",
       "                                       wikipedia_url iconic_taxon_name  \\\n",
       "0       http://en.wikipedia.org/wiki/Storeria_dekayi          Reptilia   \n",
       "1  http://en.wikipedia.org/wiki/Potentilla_norvegica           Plantae   \n",
       "2         http://en.wikipedia.org/wiki/Tettigoniinae           Insecta   \n",
       "3      http://en.wikipedia.org/wiki/Marpissa_lineata         Arachnida   \n",
       "4   https://en.wikipedia.org/wiki/Eastern_box_turtle          Reptilia   \n",
       "\n",
       "               preferred_common_name  \\\n",
       "0                 Dekay's Brownsnake   \n",
       "1                   rough cinquefoil   \n",
       "2                Shieldback Katydids   \n",
       "3  Four-lined Slender Jumping Spider   \n",
       "4                 Eastern Box Turtle   \n",
       "\n",
       "                                                 uri  longitude   latitude  \\\n",
       "0  https://www.inaturalist.org/observations/84313002 -88.367813  40.212863   \n",
       "1  https://www.inaturalist.org/observations/84311385 -88.369087  40.213030   \n",
       "2  https://www.inaturalist.org/observations/84311301 -88.368783  40.210567   \n",
       "3  https://www.inaturalist.org/observations/84306238 -88.208083  40.112786   \n",
       "4  https://www.inaturalist.org/observations/84287773 -88.164075  40.030186   \n",
       "\n",
       "                               place_guess  \n",
       "0                          Mahomet, IL, US  \n",
       "1  N Lake of the Woods Rd, Mahomet, IL, US  \n",
       "2                          Mahomet, IL, US  \n",
       "3                W Main St, Urbana, IL, US  \n",
       "4                             Illinois, US  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>time_observed_at</th>\n      <th>species_guess</th>\n      <th>genus_name</th>\n      <th>rank</th>\n      <th>wikipedia_url</th>\n      <th>iconic_taxon_name</th>\n      <th>preferred_common_name</th>\n      <th>uri</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>place_guess</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2021-06-23T16:22:12-05:00</td>\n      <td>NaN</td>\n      <td>Storeria dekayi</td>\n      <td>species</td>\n      <td>http://en.wikipedia.org/wiki/Storeria_dekayi</td>\n      <td>Reptilia</td>\n      <td>Dekay's Brownsnake</td>\n      <td>https://www.inaturalist.org/observations/84313002</td>\n      <td>-88.367813</td>\n      <td>40.212863</td>\n      <td>Mahomet, IL, US</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2021-06-24T10:29:25-05:00</td>\n      <td>NaN</td>\n      <td>Potentilla norvegica</td>\n      <td>species</td>\n      <td>http://en.wikipedia.org/wiki/Potentilla_norvegica</td>\n      <td>Plantae</td>\n      <td>rough cinquefoil</td>\n      <td>https://www.inaturalist.org/observations/84311385</td>\n      <td>-88.369087</td>\n      <td>40.213030</td>\n      <td>N Lake of the Woods Rd, Mahomet, IL, US</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2021-06-23T14:19:35-05:00</td>\n      <td>NaN</td>\n      <td>Tettigoniinae</td>\n      <td>subfamily</td>\n      <td>http://en.wikipedia.org/wiki/Tettigoniinae</td>\n      <td>Insecta</td>\n      <td>Shieldback Katydids</td>\n      <td>https://www.inaturalist.org/observations/84311301</td>\n      <td>-88.368783</td>\n      <td>40.210567</td>\n      <td>Mahomet, IL, US</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2021-06-24T09:45:31+00:00</td>\n      <td>NaN</td>\n      <td>Marpissa lineata</td>\n      <td>species</td>\n      <td>http://en.wikipedia.org/wiki/Marpissa_lineata</td>\n      <td>Arachnida</td>\n      <td>Four-lined Slender Jumping Spider</td>\n      <td>https://www.inaturalist.org/observations/84306238</td>\n      <td>-88.208083</td>\n      <td>40.112786</td>\n      <td>W Main St, Urbana, IL, US</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2021-06-24T06:03:52-05:00</td>\n      <td>Eastern Box Turtle</td>\n      <td>Terrapene carolina carolina</td>\n      <td>subspecies</td>\n      <td>https://en.wikipedia.org/wiki/Eastern_box_turtle</td>\n      <td>Reptilia</td>\n      <td>Eastern Box Turtle</td>\n      <td>https://www.inaturalist.org/observations/84287773</td>\n      <td>-88.164075</td>\n      <td>40.030186</td>\n      <td>Illinois, US</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Get taxonomy data from the rows\n",
    "\n",
    "## Search for a term in wikipedia search and get its page\n",
    "\n",
    "Uncomment for looking at results. I didn't find it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wikipedia\n",
    "# result = wikipedia.search(\"Monarda_fistulosa\")\n",
    "# print(result)\n",
    "# # get the page: Neural network\n",
    "# page = wikipedia.page(result[0])\n",
    "# print(page)\n",
    "# # get the title of the page\n",
    "# title = page.title\n",
    "# print(title)\n",
    "# # get the categories of the page\n",
    "# categories = page.categories\n",
    "# print(categories)\n",
    "# # get the whole wikipedia page text (content)\n",
    "# content = page.content\n",
    "# print(content)\n",
    "# # get all the links in the page\n",
    "# links = page.links\n",
    "# print(links)\n",
    "# # get the page references\n",
    "# references = page.references\n",
    "# print(references)\n",
    "# # summary\n",
    "# summary = page.summary\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1. Scraping data using beautiful soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row, if wikipedia_uri exists, we will use beautiful soup to extract the data \n",
    "# related to taxonomy\n",
    "# import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# get URL\n",
    "page = requests.get(\"http://en.wikipedia.org/wiki/Monarda_fistulosa\")\n",
    "  \n",
    "# display status code\n",
    "# print(page.status_code)\n",
    "  \n",
    "# display scrapped data\n",
    "# print(page.content)\n",
    "\n",
    "# scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'html.parser') #.get_text(strip=True) #This removes \\xa0\n",
    "\n",
    "# display scrapped data\n",
    "# print(soup.prettify())\n",
    "\n",
    "# list(soup.children)\n",
    "\n",
    "# find all occurance of p in HTML\n",
    "# includes HTML tags\n",
    "# print(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'http://en.wikipedia.org/wiki/Aesculus_parviflora'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "data_df[data_df['rank'] == 'species' ]['wikipedia_url'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1.1. Get table from the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Kingdom', 'Plantae'],\n",
       " ['Order', 'Lamiales'],\n",
       " ['Family', 'Lamiaceae'],\n",
       " ['Genus', 'Monarda'],\n",
       " ['Species', 'M. fistulosa']]"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "# create object\n",
    "# object = soup.find(id=\"mw-content-text\")\n",
    "\n",
    "# # find tags\n",
    "# items = object.find_all(class_=\"infobox biota\")\n",
    "# result = items[0]\n",
    "  \n",
    "# # display tags\n",
    "# print(result.prettify())\n",
    "\n",
    "table = soup.find_all('table')\n",
    "\n",
    "# table[0]\n",
    "\n",
    "# for child in soup.find_all('table')[0].children:\n",
    "#     for td in child:\n",
    "#         print(td)\n",
    "\n",
    "# list(soup.find_all('table')[0].tr.next_siblings)\n",
    "\n",
    "# for sibling in soup.find_all('table')[0].tr.next_siblings:\n",
    "#     for td in sibling:\n",
    "#         print(td)\n",
    "\n",
    "table = soup.find('table', attrs={'class':'infobox biota'}) #class=\"infobox biota\"\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "# table_rows\n",
    "\n",
    "# len(table_rows)\n",
    "data_taxonomy = {\n",
    "    'Kingdom': list(),\n",
    "    'Phylum': list(),\n",
    "    'Class': list(),\n",
    "    'Order': list(),\n",
    "    'Suborder': list(),\n",
    "    'Family': list(),\n",
    "    'Genus': list(),\n",
    "    'Species': list()\n",
    "}\n",
    "l = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [tr.text.replace('\\n', '').replace(':', '').replace(u'\\xa0', ' ') for tr in td]\n",
    "    data_taxonomy['Kingdom'].append(None)\n",
    "    data_taxonomy['Phylum'].append(None)\n",
    "    data_taxonomy['Class'].append(None)\n",
    "    data_taxonomy['Order'].append(None)\n",
    "    data_taxonomy['Suborder'].append(None)\n",
    "    data_taxonomy['Family'].append(None)\n",
    "    data_taxonomy['Genus'].append(None)\n",
    "    data_taxonomy['Species'].append(None)\n",
    "\n",
    "    if 'Kingdom' in row:\n",
    "        data_taxonomy['Kingdom'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Phylum' in row:\n",
    "        data_taxonomy['Phylum'] = row[1]\n",
    "        l.append(row)   \n",
    "    if 'Class' in row:\n",
    "        data_taxonomy['Class'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Order' in row:\n",
    "        data_taxonomy['Order'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Suborder' in row:\n",
    "        data_taxonomy['Suborder'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Family' in row:\n",
    "        data_taxonomy['Family'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Genus' in row:\n",
    "        data_taxonomy['Genus'] = row[1]\n",
    "        l.append(row)\n",
    "    if 'Species' in row:\n",
    "        data_taxonomy['Species'] = row[1]\n",
    "        l.append(row)\n",
    "    \n",
    "    # elif 'Clade' in row:\n",
    "    #     l.append(row)\n",
    "l\n",
    "\n",
    "# 'Kingdom' in l[4]\n",
    "\n",
    "# table_rows = table_rows[4:13]\n",
    "\n",
    "# l = []\n",
    "# for tr in table_rows:\n",
    "#     td = tr.find_all('td')\n",
    "#     row = [tr.text for tr in td]\n",
    "#     l.append(row)\n",
    "\n",
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Kingdom': ['Plantae'],\n",
       " 'Phylum': [],\n",
       " 'Class': [],\n",
       " 'Order': ['Lamiales'],\n",
       " 'Suborder': [],\n",
       " 'Family': ['Lamiaceae'],\n",
       " 'Genus': ['Monarda'],\n",
       " 'Species': ['M. fistulosa']}"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "data_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Kingdom', 'Plantae']\n['Clade', 'Tracheophytes']\n['Clade', 'Angiosperms']\n['Clade', 'Eudicots']\n['Clade', 'Asterids']\n['Order', 'Lamiales']\n['Family', 'Lamiaceae']\n['Genus', 'Monarda']\n['Species', 'M. fistulosa']\n"
     ]
    }
   ],
   "source": [
    "for item in l:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = {'taxonomy': list()}\n",
    "\n",
    "taxonomy['taxonomy'].append({item[0]: item[1] for item in l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'taxonomy': [{'Kingdom': 'Plantae',\n",
       "   'Clade': 'Asterids',\n",
       "   'Order': 'Lamiales',\n",
       "   'Family': 'Lamiaceae',\n",
       "   'Genus': 'Monarda',\n",
       "   'Species': 'M. fistulosa'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "1. Put dictionary in database\n",
    "    a. Use something like Unstructured database like MongoDB.\n",
    "        * Each record is a dictionary.\n",
    "        \n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd07812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}